"""
Professional Lambda-based RAG Indexing Service
Handles document processing in /tmp with proper error handling and monitoring
"""

import json
import os
import tempfile
import logging
import boto3
import time
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import traceback

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class IndexingConfig:
    """Configuration for indexing job"""
    s3_bucket: str
    s3_pdfs_prefix: str
    s3_cache_prefix: str
    max_processing_time: int = 840  # 14 minutes (leave 1 min buffer)
    batch_size: int = 3  # Process 3 PDFs at a time
    max_memory_mb: int = 2800  # Leave buffer for Lambda
    min_text_chars: int = 300  # Skip PDFs with less than this many chars (assume OCR needed)
    
@dataclass
class IndexingResult:
    """Result of indexing operation"""
    success: bool
    documents_processed: int
    total_documents: int
    processing_time: float
    cache_size_mb: float
    error_message: Optional[str] = None
    files_created: List[str] = None

class ResourceMonitor:
    """Monitor Lambda resources during processing"""
    
    def __init__(self):
        self.start_time = time.time()
        self.memory_limit_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '3008'))
        
    def check_time_remaining(self) -> float:
        """Return seconds remaining before timeout"""
        elapsed = time.time() - self.start_time
        return 900 - elapsed  # 15 minutes - elapsed
        
    def check_memory_usage(self) -> Dict[str, float]:
        """Check memory usage (simplified for Lambda)"""
        # Prefer psutil if available for accurate process RSS
        try:
            import psutil  # type: ignore
            process = psutil.Process()
            memory_info = process.memory_info()
            used_mb = memory_info.rss / 1024 / 1024
            return {
                'used_mb': used_mb,
                'available_mb': max(0.0, self.memory_limit_mb - used_mb)
            }
        except Exception:
            # Fallback 1: parse /proc/self/status (VmRSS in kB)
            try:
                with open('/proc/self/status', 'r') as f:
                    for line in f:
                        if line.startswith('VmRSS:'):
                            parts = line.split()
                            # Example: VmRSS:\t  123456 kB
                            rss_kb = float(parts[1])
                            used_mb = rss_kb / 1024.0
                            return {
                                'used_mb': used_mb,
                                'available_mb': max(0.0, self.memory_limit_mb - used_mb)
                            }
            except Exception:
                pass

            # Fallback 2: parse /proc/meminfo for system available; use conservative estimate
            try:
                mem_total_mb = None
                mem_available_mb = None
                with open('/proc/meminfo', 'r') as f:
                    for line in f:
                        if line.startswith('MemTotal:'):
                            mem_total_mb = float(line.split()[1]) / 1024.0
                        elif line.startswith('MemAvailable:'):
                            mem_available_mb = float(line.split()[1]) / 1024.0
                if mem_total_mb is not None and mem_available_mb is not None:
                    used_mb = max(0.0, mem_total_mb - mem_available_mb)
                    # Bound by Lambda function memory limit
                    available_mb = max(0.0, min(self.memory_limit_mb, mem_available_mb))
                    return {
                        'used_mb': used_mb,
                        'available_mb': available_mb
                    }
            except Exception:
                pass

            # Last resort: return conservative values so processing can continue
            # Assume small usage and keep a fixed safety buffer
            used_mb = 128.0
            available_mb = max(0.0, self.memory_limit_mb - used_mb)
            return {
                'used_mb': used_mb,
                'available_mb': available_mb
            }
        
    def should_continue(self) -> bool:
        """Check if we should continue processing"""
        time_remaining = self.check_time_remaining()
        memory_info = self.check_memory_usage()
        
        return (
            time_remaining > 120 and  # At least 2 minutes left
            memory_info['available_mb'] > 500  # At least 500MB available
        )

class S3Manager:
    """Professional S3 operations with error handling"""
    
    def __init__(self, bucket: str):
        self.bucket = bucket
        self.s3_client = boto3.client('s3')
        
    def list_pdfs(self, prefix: str) -> List[str]:
        """List all PDF files in S3 prefix"""
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket,
                Prefix=prefix,
                MaxKeys=1000
            )
            
            pdf_files = []
            for obj in response.get('Contents', []):
                if obj['Key'].lower().endswith('.pdf'):
                    pdf_files.append(obj['Key'])
                    
            logger.info(f"Found {len(pdf_files)} PDF files in s3://{self.bucket}/{prefix}")
            return pdf_files
            
        except Exception as e:
            logger.error(f"Error listing PDFs: {str(e)}")
            raise
            
    def download_file(self, s3_key: str, local_path: str) -> bool:
        """Download file from S3 with retry logic"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Downloading s3://{self.bucket}/{s3_key} to {local_path}")
                self.s3_client.download_file(self.bucket, s3_key, local_path)
                return True
                
            except Exception as e:
                logger.warning(f"Download attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries - 1:
                    logger.error(f"Failed to download {s3_key} after {max_retries} attempts")
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff
                
        return False
        
    def upload_file(self, local_path: str, s3_key: str) -> bool:
        """Upload file to S3 with retry logic"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Uploading {local_path} to s3://{self.bucket}/{s3_key}")
                self.s3_client.upload_file(local_path, self.bucket, s3_key)
                return True
                
            except Exception as e:
                logger.warning(f"Upload attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries - 1:
                    logger.error(f"Failed to upload {s3_key} after {max_retries} attempts")
                    raise
                time.sleep(2 ** attempt)
                
        return False
        
    def upload_json(self, data: Dict, s3_key: str) -> bool:
        """Upload JSON data directly to S3"""
        try:
            logger.info(f"Uploading JSON to s3://{self.bucket}/{s3_key}")
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=s3_key,
                Body=json.dumps(data, indent=2),
                ContentType='application/json'
            )
            return True
        except Exception as e:
            logger.error(f"Failed to upload JSON {s3_key}: {str(e)}")
            raise

class RAGIndexer:
    """Professional RAG indexing with batching and optimization"""
    
    def __init__(self, temp_dir: str, config: IndexingConfig):
        self.temp_dir = temp_dir
        self.config = config
        self.monitor = ResourceMonitor()
        
        # Import heavy dependencies only when needed
        self._vector_store = None
        self._embeddings = None
        
    def _load_dependencies(self):
        """Lazy load heavy ML dependencies"""
        if self._vector_store is None:
            logger.info("Loading vector store dependencies...")
            # Import your enhanced_rag_qwen components here
            # from enhanced_rag_qwen import EnhancedRAG
            # self._vector_store = EnhancedRAG()
            
    def process_batch(self, pdf_files: List[str], s3_manager: S3Manager) -> Dict[str, Any]:
        """Process a batch of PDFs with memory optimization"""
        batch_result = {
            'processed': 0,
            'failed': [],
            'documents': []
        }
        
        for pdf_file in pdf_files:
            if not self.monitor.should_continue():
                logger.warning("Resource limits reached, stopping batch processing")
                break
                
            try:
                # Download PDF to temp location
                local_pdf = os.path.join(self.temp_dir, os.path.basename(pdf_file))
                s3_manager.download_file(pdf_file, local_pdf)
                
                # Process PDF (implement your PDF processing logic)
                documents = self._process_single_pdf(local_pdf)
                batch_result['documents'].extend(documents)
                batch_result['processed'] += 1
                
                # Clean up immediately to save space
                os.remove(local_pdf)
                logger.info(f"Processed and cleaned up {pdf_file}")
                
            except Exception as e:
                logger.error(f"Failed to process {pdf_file}: {str(e)}")
                batch_result['failed'].append({
                    'file': pdf_file,
                    'error': str(e)
                })
                
        return batch_result
        
    def _process_single_pdf(self, pdf_path: str) -> List[Dict]:
        """Process single PDF and extract text using PyMuPDF, fallback to PDFMiner/PyPDF2"""
        import io
        import fitz  # PyMuPDF
        from pdfminer.high_level import extract_text as pdfminer_extract_text
        import PyPDF2
        
        documents = []
        logger.info(f"Processing PDF: {pdf_path}")
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_bytes = file.read()
                full_text = ""

                # Quick sample gate with PyMuPDF + PDFMiner for page 0
                try:
                    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
                        n_pages = doc.page_count
                        sample_pages = min(self.config.batch_size + 4, n_pages)  # small sample
                        sample_has_text = False
                        for i in range(sample_pages):
                            try:
                                ptxt = doc.load_page(i).get_text("text", sort=True) or ""
                                if len(ptxt.strip()) >= self.config.min_text_chars:
                                    sample_has_text = True
                                    break
                            except Exception:
                                continue
                except Exception as e:
                    logger.warning(f"PyMuPDF open failed, will try PDFMiner sample: {e}")
                    sample_has_text = False

                if not sample_has_text:
                    try:
                        miner_sample = pdfminer_extract_text(io.BytesIO(pdf_bytes), page_numbers=[0]) or ""
                        if len(miner_sample.strip()) >= self.config.min_text_chars:
                            sample_has_text = True
                    except Exception:
                        pass

                if not sample_has_text:
                    logger.warning(f"Insufficient text in sample. Skipping {pdf_path}")
                    return []

                # Per-page extraction (PyMuPDF), keep only pages with enough text
                try:
                    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
                        for page_num in range(doc.page_count):
                            try:
                                ptxt = doc.load_page(page_num).get_text("text", sort=True) or ""
                                if len(ptxt.strip()) >= self.config.min_text_chars:
                                    full_text += f"\n--- Page {page_num + 1} ---\n{ptxt}"
                            except Exception as e:
                                logger.warning(f"PyMuPDF failed on page {page_num+1}: {e}")
                except Exception as e:
                    logger.warning(f"PyMuPDF per-page failed, fallback to PDFMiner full: {e}")
                    try:
                        full_text = pdfminer_extract_text(io.BytesIO(pdf_bytes)) or ""
                    except Exception as e2:
                        logger.warning(f"PDFMiner full-doc failed, fallback to PyPDF2: {e2}")
                        try:
                            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
                            for page_num, page in enumerate(pdf_reader.pages):
                                try:
                                    page_text = page.extract_text()
                                    if page_text and len(page_text.strip()) >= self.config.min_text_chars:
                                        full_text += f"\n--- Page {page_num + 1} ---\n{page_text}"
                                except Exception:
                                    continue
                        except Exception:
                            pass
                
                text_stripped = full_text.strip()
                if text_stripped:
                    if len(text_stripped) < self.config.min_text_chars:
                        logger.warning(
                            f"Insufficient text extracted (chars={len(text_stripped)} < {self.config.min_text_chars}). "
                            f"Likely scanned/OCR-only. Skipping {pdf_path}"
                        )
                        return []
                    # Clean and chunk the text
                    cleaned_text = self._clean_text(text_stripped)
                    chunks = self._chunk_text(cleaned_text, pdf_path)
                    documents.extend(chunks)
                    
                    logger.info(f"Extracted {len(chunks)} chunks from {pdf_path}")
                else:
                    logger.warning(f"No text extracted from {pdf_path}")
                    
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_path}: {str(e)}")
            
        return documents
    
    def _clean_text(self, text: str) -> str:
        """Clean extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove page markers
        text = re.sub(r'--- Page \d+ ---', '', text)
        # Remove special characters that might cause issues
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)]', ' ', text)
        return text.strip()
    
    def _chunk_text(self, text: str, source_file: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict]:
        """Split text into overlapping chunks"""
        chunks = []
        
        # Simple sentence-aware chunking
        sentences = re.split(r'[.!?]+', text)
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # If adding this sentence would exceed chunk size, start new chunk
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append({
                    'content': current_chunk.strip(),
                    'source': source_file,
                    'chunk_id': len(chunks),
                    'char_count': len(current_chunk)
                })
                
                # Start new chunk with overlap
                words = current_chunk.split()
                if len(words) > overlap // 10:  # Rough word-based overlap
                    current_chunk = ' '.join(words[-(overlap // 10):]) + ' ' + sentence
                else:
                    current_chunk = sentence
            else:
                current_chunk += ' ' + sentence
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'source': source_file,
                'chunk_id': len(chunks),
                'char_count': len(current_chunk)
            })
        
        return chunks
        
    def build_indexes(self, documents: List[Dict]) -> Dict[str, str]:
        """Build lightweight hybrid BM25 + TF-IDF indexes using custom implementation"""
        logger.info(f"Building lightweight hybrid indexes from {len(documents)} documents")
        
        if not documents:
            logger.warning("No documents to index")
            return {}
        
        try:
            # Import minimal dependencies (avoid heavy native libs like numpy)
            import pickle
            import gzip
            import json
            import math
            import re
            from collections import Counter, defaultdict
            
            # Extract text content for indexing
            texts = [doc['content'] for doc in documents]
            
            # Custom lightweight TF-IDF implementation
            logger.info("Building custom TF-IDF vectors...")
            
            # Tokenize and preprocess
            def simple_tokenize(text):
                # Simple tokenization with stopword removal
                stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'this', 'that', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}
                tokens = re.findall(r'\b[a-z]{3,}\b', text.lower())
                return [token for token in tokens if token not in stopwords]
            
            # Build vocabulary and term frequencies
            vocabulary = set()
            doc_tokens = []
            for text in texts:
                tokens = simple_tokenize(text)
                doc_tokens.append(tokens)
                vocabulary.update(tokens)
            
            vocab_list = sorted(list(vocabulary))
            vocab_size = len(vocab_list)
            vocab_index = {word: i for i, word in enumerate(vocab_list)}
            
            logger.info(f"Built vocabulary: {vocab_size} terms")
            
            # Calculate TF-IDF as sparse vectors (pure Python, no numpy)
            num_docs = len(texts)
            
            # Document frequency for IDF calculation
            df = defaultdict(int)
            for tokens in doc_tokens:
                for token in set(tokens):
                    df[token] += 1
            
            # Precompute IDF with smoothing
            idf_by_token = {}
            for token in vocabulary:
                idf_by_token[token] = math.log(num_docs / (1 + df[token]))
            
            # Build normalized sparse TF-IDF vectors for each document
            sparse_tfidf_by_doc = []  # list[dict[token_idx, weight]]
            for tokens in doc_tokens:
                if not tokens:
                    sparse_tfidf_by_doc.append({})
                    continue
                counts = Counter(tokens)
                weights = {}
                # compute squared norm
                sum_sq = 0.0
                for token, count in counts.items():
                    if token not in vocab_index:
                        continue
                    tf = 1 + math.log(count) if count > 0 else 0.0
                    idf = idf_by_token.get(token, 0.0)
                    w = tf * idf
                    if w == 0.0:
                        continue
                    idx = vocab_index[token]
                    weights[idx] = w
                    sum_sq += w * w
                # L2 normalize
                if sum_sq > 0:
                    norm = math.sqrt(sum_sq)
                    for idx in list(weights.keys()):
                        weights[idx] = weights[idx] / norm
                sparse_tfidf_by_doc.append(weights)
            
            # For compatibility, we still produce two artifacts, but both use the sparse TF-IDF
            logger.info("Built sparse TF-IDF representations without numpy")
            
            # Define file paths (keeping same structure for compatibility)
            index_files = {
                'vector_index': os.path.join(self.temp_dir, 'vector_index.pkl'),
                'bm25_index': os.path.join(self.temp_dir, 'bm25_index.pkl'),
                'documents': os.path.join(self.temp_dir, 'documents.pkl.gz'),
                'manifest': os.path.join(self.temp_dir, 'manifest.json')
            }
            
            # Save vector index (sparse TF-IDF)
            vector_data = {
                'vectors': sparse_tfidf_by_doc,
                'vocabulary': vocab_list,
                'dimension': len(vocab_list),
                'idf': idf_by_token,
                'index_type': 'custom_tfidf_sparse',
                'version': '1.0'
            }
            with open(index_files['vector_index'], 'wb') as f:
                pickle.dump(vector_data, f)
            
            # Save BM25-style index placeholder using same sparse TF-IDF
            bm25_data = {
                'vectors': sparse_tfidf_by_doc,
                'vocabulary': vocab_list,
                'vocab_index': vocab_index,
                'idf': idf_by_token,
                'index_type': 'custom_bm25_tfidf_sparse',
                'version': '1.0'
            }
            with open(index_files['bm25_index'], 'wb') as f:
                pickle.dump(bm25_data, f)
            
            # Save documents (compressed)
            with gzip.open(index_files['documents'], 'wb') as f:
                pickle.dump(documents, f)
            
            # Create manifest
            manifest = {
                'documents_count': len(documents),
                'embedding_dimension': len(vocab_list),
                'vocabulary_size': vocab_size,
                'model_name': 'custom_lightweight_tfidf_sparse',
                'created_at': time.time(),
                'index_files': list(index_files.keys()),
                'search_type': 'hybrid_custom_lightweight',
                'lambda_compatible': True,
                'package_size_optimized': True
            }
            
            with open(index_files['manifest'], 'w') as f:
                json.dump(manifest, f, indent=2)
            
            logger.info(f"Successfully built lightweight indexes: {len(documents)} documents, "
                       f"vocab={vocab_size}, representation=sparse TF-IDF")
            return index_files
            
        except Exception as e:
            logger.error(f"Error building indexes: {str(e)}")
            raise

def create_manifest(config: IndexingConfig, result: IndexingResult) -> Dict[str, Any]:
    """Create manifest file for cache compatibility"""
    return {
        'version': '1.0',
        'created_at': datetime.utcnow().isoformat(),
        'documents_count': result.documents_processed,
        'total_size_mb': result.cache_size_mb,
        'processing_time_seconds': result.processing_time,
        'config': {
            'batch_size': config.batch_size,
            'max_memory_mb': config.max_memory_mb
        },
        'files': result.files_created or []
    }

def lambda_handler(event: Dict[str, Any], context) -> Dict[str, Any]:
    """Main Lambda handler with comprehensive error handling"""
    
    start_time = time.time()
    logger.info(f"Starting RAG indexing job with event: {json.dumps(event)}")
    
    try:
        # Fast-path health check to support CI smoke tests
        if isinstance(event, dict):
            action = str(event.get('action', '')).lower()
            if event.get('test') or event.get('ping') or event.get('dry_run') or action in ('health', 'test'):
                return {
                    'statusCode': 200,
                    'body': json.dumps({
                        'success': True,
                        'message': 'lambda alive',
                        'timestamp': time.time()
                    })
                }

        # Parse configuration
        config = IndexingConfig(
            s3_bucket=os.environ['S3_BUCKET'],
            s3_pdfs_prefix=os.environ.get('S3_PDFS_PREFIX', 'rag_pdfs/'),
            s3_cache_prefix=os.environ.get('S3_CACHE_PREFIX', 'cache/current/'),
            batch_size=int(os.environ.get('BATCH_SIZE', '3')),
            max_memory_mb=int(os.environ.get('MAX_MEMORY_MB', '2800')),
            min_text_chars=int(os.environ.get('MIN_TEXT_CHARS', '300'))
        )
        
        # Initialize services
        s3_manager = S3Manager(config.s3_bucket)
        
        # Use Lambda's /tmp directory
        with tempfile.TemporaryDirectory(dir='/tmp') as temp_dir:
            logger.info(f"Using temporary directory: {temp_dir}")
            
            # Initialize indexer
            indexer = RAGIndexer(temp_dir, config)
            
            # Get list of PDFs to process
            pdf_files = s3_manager.list_pdfs(config.s3_pdfs_prefix)
            
            if not pdf_files:
                return {
                    'statusCode': 200,
                    'body': json.dumps({
                        'success': True,
                        'message': 'No PDFs found to process'
                    })
                }
            
            # Process PDFs in batches
            all_documents = []
            total_processed = 0
            
            for i in range(0, len(pdf_files), config.batch_size):
                batch = pdf_files[i:i + config.batch_size]
                logger.info(f"Processing batch {i//config.batch_size + 1}: {len(batch)} files")
                
                batch_result = indexer.process_batch(batch, s3_manager)
                all_documents.extend(batch_result['documents'])
                total_processed += batch_result['processed']
                
                if batch_result['failed']:
                    logger.warning(f"Batch had {len(batch_result['failed'])} failures")
            
            # Build indexes
            index_files = indexer.build_indexes(all_documents)
            
            # Upload indexes to S3
            uploaded_files = []
            for index_name, file_path in index_files.items():
                if os.path.exists(file_path):
                    s3_key = f"{config.s3_cache_prefix}{os.path.basename(file_path)}"
                    s3_manager.upload_file(file_path, s3_key)
                    uploaded_files.append(s3_key)
            
            # Create and upload manifest
            processing_time = time.time() - start_time
            cache_size_mb = sum(os.path.getsize(f) for f in index_files.values() if os.path.exists(f)) / 1024 / 1024
            
            result = IndexingResult(
                success=True,
                documents_processed=total_processed,
                total_documents=len(pdf_files),
                processing_time=processing_time,
                cache_size_mb=cache_size_mb,
                files_created=uploaded_files
            )
            
            manifest = create_manifest(config, result)
            s3_manager.upload_json(manifest, f"{config.s3_cache_prefix}manifest.json")
            
            logger.info(f"Indexing completed successfully: {total_processed}/{len(pdf_files)} files processed")
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'success': True,
                    'documents_processed': total_processed,
                    'total_documents': len(pdf_files),
                    'processing_time': processing_time,
                    'cache_size_mb': cache_size_mb,
                    'files_uploaded': len(uploaded_files)
                })
            }
            
    except Exception as e:
        error_message = f"Indexing failed: {str(e)}"
        logger.error(error_message)
        logger.error(traceback.format_exc())
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'success': False,
                'error': error_message,
                'processing_time': time.time() - start_time
            })
        }